@misc{kautsarSEADialoguesMultilingualCulturally2025,
  batch = {2025},
  title = {{{SEADialogues}}: {{A Multilingual Culturally Grounded Multi-turn Dialogue Dataset}} on {{Southeast Asian Languages}}},
  shorttitle = {{{SEADialogues}}},
  author = {Kautsar, Muhammad Dehan Al and Candra, Aswin and Hakim, Muhammad Alif Al and Kahfi, Maxalmina Satria and Koto, Fajri and Aji, Alham Fikri and Limkonchotiwat, Peerat and Chuangsuwanich, Ekapol and Winata, Genta Indra},
  year = {2025},
  month = aug,
  number = {arXiv:2508.07069},
  eprint = {2508.07069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.07069},
  url = {http://arxiv.org/abs/2508.07069},
  pdf = {https://arxiv.org/pdf/2508.07069},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  abstract = {Although numerous datasets have been developed to support dialogue systems, most existing chit-chat datasets overlook the cultural nuances inherent in natural human conversations. To address this gap, we introduce SEADialogues, a culturally grounded dialogue dataset centered on Southeast Asia, a region with over 700 million people and immense cultural diversity. Our dataset features dialogues in eight languages from six Southeast Asian countries, many of which are low-resource despite having sizable speaker populations. To enhance cultural relevance and personalization, each dialogue includes persona attributes and two culturally grounded topics that reflect everyday life in the respective communities. Furthermore, we release a multi-turn dialogue dataset to advance research on culturally aware and human-centric large language models, including conversational dialogue agents.},
  resources={https://huggingface.co/datasets/SEACrowd/SEADialogues}
}

@inproceedings{lopo-etal-2025-language,
  batch = {2025},
  title = "Language Surgery in Multilingual Large Language Models",
  author = "Lopo, Joanito Agili  and
    Habibi, Muhammad Ravi Shulthan  and
    Wong, Tack Hwa  and
    Ghozali, Muhammad Ilham  and
    Koto, Fajri  and
    Winata, Genta Indra  and
    Limkonchotiwat, Peerat  and
    Aji, Alham Fikri  and
    Cahyawijaya, Samuel",
  editor = "Adelani, David Ifeoluwa  and
    Arnett, Catherine  and
    Ataman, Duygu  and
    Chang, Tyler A.  and
    Gonen, Hila  and
    Raja, Rahul  and
    Schmidt, Fabian  and
    Stap, David  and
    Wang, Jiayi",
  booktitle = "Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025)",
  month = nov,
  year = "2025",
  address = "Suzhuo, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2025.mrl-main.30/",
  doi = "10.18653/v1/2025.mrl-main.30",
  pages = "438--467",
  ISBN = "979-8-89176-345-6",
  abstract = "Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC{'}s strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their monolingual and cross-lingual performance."
}

@inproceedings{irawan-etal-2025-entropy2vec,
  batch = {2025},
  title = "{E}ntropy2{V}ec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations",
  author = "Irawan, Patrick Amadeus  and
    Diandaru, Ryandito  and
    Syuhada, Belati Jagad Bintang  and
    Suchrady, Randy Zakya  and
    Aji, Alham Fikri  and
    Winata, Genta Indra  and
    Koto, Fajri  and
    Cahyawijaya, Samuel",
  editor = "Adelani, David Ifeoluwa  and
    Arnett, Catherine  and
    Ataman, Duygu  and
    Chang, Tyler A.  and
    Gonen, Hila  and
    Raja, Rahul  and
    Schmidt, Fabian  and
    Stap, David  and
    Wang, Jiayi",
  booktitle = "Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025)",
  month = nov,
  year = "2025",
  address = "Suzhuo, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2025.mrl-main.29/",
  doi = "10.18653/v1/2025.mrl-main.29",
  pages = "426--437",
  ISBN = "979-8-89176-345-6",
  abstract = "We introduce Entropy2Vec, a novel framework for deriving cross-lingual language representations by leveraging the entropy of monolingual language models. Unlike traditional typological inventories that suffer from feature sparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in language models to capture typological relationships between languages. By training a language model on a single language, we hypothesize that the entropy of its predictions reflects its structural similarity to other languages: Low entropy indicates high similarity, while high entropy suggests greater divergence. This approach yields dense, non-sparse language embeddings that are adaptable to different timeframes and free from missing values. Empirical evaluations demonstrate that Entropy2Vec embeddings align with established typological categories and achieved competitive performance in downstream multilingual NLP tasks, such as those addressed by the LinguAlchemy framework."
}
